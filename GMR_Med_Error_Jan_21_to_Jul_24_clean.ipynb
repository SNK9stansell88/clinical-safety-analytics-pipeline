{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jk5cxivHvm0a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVMTL8L8BnWi"
   },
   "source": [
    "# Step 1 — Data Loading and Basic Structure (Medication Dataset)\n",
    "\n",
    "In this step, I load the two datasets needed for medication-error analysis:\n",
    "• The **Medication** sheet, containing row-level event details.\n",
    "• The **Med Error Summary** sheet, containing certificate-level counts by categories such as Pattern Specifics.\n",
    "\n",
    "The purpose of Step 1 is to:\n",
    "• Ensure the Excel file loads correctly.\n",
    "• Confirm the number of rows and columns in each sheet.\n",
    "• Display the first few rows to understand structure and data types.\n",
    "• Verify that critical variables exist, including:\n",
    "  – Pattern Specifics (our main category of interest)\n",
    "  – Certificate columns (AEL, GLF, AMR, MTC, REACH, etc.)\n",
    "  – Event-level fields such as Branch, Source, Primary Risk, Risk Event, Medications, etc.\n",
    "\n",
    "This mirrors Step 1 of the Loan Assignment (data loading and inspection), but adapted to a clinical medication-error context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BQr8OsSkBtBS",
    "outputId": "dc9cc6d5-3e72-45c4-86b0-9588812be188"
   },
   "outputs": [],
   "source": [
    "# Step 1 — Load Medication + Summary Sheets Cleanly\n",
    "\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "\n",
    "# Upload the Krista file (do this ONCE)\n",
    "uploaded = files.upload()   # Select \"Krista 240726 Final.xlsx\"\n",
    "\n",
    "# Load the workbook\n",
    "file_name = \"Krista 240726 Final.xlsx\"\n",
    "\n",
    "# Read both sheets\n",
    "med = pd.read_excel(file_name, sheet_name=\"Medication\")\n",
    "med_summary = pd.read_excel(file_name, sheet_name=\"Med Error Summary\")\n",
    "\n",
    "# Basic structure\n",
    "print(\"Medication sheet shape:\", med.shape)\n",
    "print(\"Med Error Summary sheet shape:\", med_summary.shape)\n",
    "\n",
    "print(\"\\nMedication sheet – first 5 rows:\")\n",
    "display(med.head())\n",
    "\n",
    "print(\"\\nMed Error Summary sheet – first 5 rows:\")\n",
    "display(med_summary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyPj0GVvB9UM"
   },
   "source": [
    "#Step 2. Exploratory Data Analysis (EDA) – Univariate Patterns (Medication Dataset)\n",
    "\n",
    "In this step, I examine the Medication sheet one variable at a time to understand the basic structure of the data before moving into more complex relationships.\n",
    "\n",
    "The specific goals for this step are:\n",
    "\n",
    "• Identify the main numeric fields in the medication dataset and look at their distributions using histograms.  \n",
    "• Examine the frequency of key categorical variables that are important for medication-error analysis, including:\n",
    "  – Certificate / system grouping (if present in this sheet or in the summary sheet)  \n",
    "  – Branch (Air vs Ground)  \n",
    "  – Medication 1 (primary medication field)  \n",
    "  – Pattern (high-level pattern label)  \n",
    "  – Pattern Specifics (the detailed pattern classification I will use instead of Risk Event)\n",
    "\n",
    "Because `Pattern Specifics` is a free-text or semi-structured field with many unique values, plotting every single value would not be very interpretable. Instead, I will:\n",
    "\n",
    "• Count how often each `Pattern Specifics` value appears.  \n",
    "• Focus on the most frequent patterns to see which failure modes show up repeatedly in the data.  \n",
    "\n",
    "This mirrors the univariate EDA step in the loan assignment (where we examined income, age, etc.), but here the focus is on medication-error patterns and the narrative fields that describe how errors occur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TvJuASyFC4nu",
    "outputId": "de833bb0-3d40-41cf-abf3-08f6214a27c2"
   },
   "outputs": [],
   "source": [
    "# STEP 2 – Exploratory Data Analysis (EDA) – Univariate Patterns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# ---- 2.1 Identify numeric columns automatically ----\n",
    "num_cols = med.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "print(\"Numeric columns detected:\", num_cols)\n",
    "\n",
    "if len(num_cols) > 0:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    n_cols = 3\n",
    "    n_rows = (len(num_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "    for i, col in enumerate(num_cols, 1):\n",
    "        plt.subplot(n_rows, n_cols, i)\n",
    "        sns.histplot(med[col], bins=30, kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numeric columns found to plot for histograms.\")\n",
    "\n",
    "# ---- 2.2 Categorical variables (including Pattern and Pattern Specifics) ----\n",
    "candidate_cat_cols = [\n",
    "    \"Certificate\",        # only used if present\n",
    "    \"Branch\",\n",
    "    \"Medication 1\",\n",
    "    \"Pattern\",\n",
    "    \"Pattern Specifics\"\n",
    "]\n",
    "\n",
    "cat_cols = [c for c in candidate_cat_cols if c in med.columns]\n",
    "print(\"Categorical columns used for EDA:\", cat_cols)\n",
    "\n",
    "for col in cat_cols:\n",
    "    # For high-cardinality columns like Pattern Specifics, show only the top 15\n",
    "    value_counts = med[col].value_counts().head(15)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(\n",
    "        x=value_counts.values,\n",
    "        y=value_counts.index,\n",
    "        orient=\"h\"\n",
    "    )\n",
    "    plt.title(f\"Top {len(value_counts)} categories for {col}\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIo8CJFbVaGh"
   },
   "source": [
    "# Step 3 — Data Overview with Emphasis on Pattern Specifics\n",
    "\n",
    "In this step, I review the overall structure of the **Medication** dataset, with a specific focus on the **Pattern Specifics** column as the main error-type variable.\n",
    "\n",
    "The goals of this step are:\n",
    "1.  **Confirm the overall size of the dataset:**\n",
    "    * How many total medication-related records are present?\n",
    "    * How many columns are included?\n",
    "2.  **List all column names** to understand what information is available for later analysis.\n",
    "3.  **Examine basic completeness and content of the *Pattern Specifics* column:**\n",
    "    * Identify how many records have valid, specific pattern data (excluding missing or \"Not documented\" entries).\n",
    "    * List the **Top 10 most frequent error patterns**.\n",
    "4.  **Briefly review a few example rows** to contextualize the error descriptions.\n",
    "\n",
    "This mirrors the “data overview” step from the loan assignment, but here the primary focus is on the clinical error description stored in *Pattern Specifics*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tiWQnwXkVdmx",
    "outputId": "9a94b487-5acc-435f-e8e7-73ace63651bc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 3: Data Overview & Pattern Specifics Inspection\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Load the Data\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    df_med = pd.read_excel('Krista 240726 Final.xlsx', sheet_name='Medication')\n",
    "except FileNotFoundError:\n",
    "    df_med = pd.read_csv('Krista 240726 Final.xlsx - Medication.csv')\n",
    "\n",
    "# 2. Confirm Dataset Size and Structure\n",
    "# ---------------------------------------------------------\n",
    "rows, cols = df_med.shape\n",
    "print(f\"--- Dataset Dimensions ---\")\n",
    "print(f\"Total Records: {rows}\")\n",
    "print(f\"Total Columns: {cols}\")\n",
    "print(\"\\n--- Column Names ---\")\n",
    "print(list(df_med.columns))\n",
    "\n",
    "# 3. Analyze the 'Pattern Specifics' Column (Excluding Missing/Unspecified)\n",
    "# ---------------------------------------------------------\n",
    "# Create a filtered series that drops NaNs and specific \"missing\" text\n",
    "valid_patterns = df_med['Pattern Specifics'].dropna()\n",
    "# Filter out common non-specific placeholders if they exist\n",
    "valid_patterns = valid_patterns[~valid_patterns.isin(['Not documented', 'Not Specified', 'Missing'])]\n",
    "\n",
    "total_patterns = len(valid_patterns)\n",
    "missing_count = rows - total_patterns\n",
    "\n",
    "print(\"\\n--- 'Pattern Specifics' Completeness ---\")\n",
    "print(f\"Valid Pattern Records: {total_patterns}\")\n",
    "print(f\"Missing/Unspecified:   {missing_count}\")\n",
    "print(f\"Completeness:          {100 * total_patterns / rows:.1f}%\")\n",
    "\n",
    "print(\"\\n--- Top 10 Most Frequent Error Patterns (Valid Only) ---\")\n",
    "print(valid_patterns.value_counts().head(10))\n",
    "\n",
    "# 4. Review Example Rows (Valid Patterns Only)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- Example Rows (First 5 with Valid Patterns) ---\")\n",
    "# Filter the main dataframe to show examples that actually have patterns\n",
    "df_examples = df_med[df_med['Pattern Specifics'].isin(valid_patterns)]\n",
    "cols_to_show = ['Source', 'Medication 1', 'Pattern', 'Pattern Specifics']\n",
    "\n",
    "# Display\n",
    "print(df_examples[cols_to_show].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn7sRtKKV5Oc"
   },
   "source": [
    "# Step 4 — Basic Exploratory Data Analysis (EDA) With Emphasis on Pattern Specifics\n",
    "\n",
    "In this step, I begin exploring the **Medication** dataset to understand the basic characteristics of the data and identify early patterns that may be useful for later modeling.\n",
    "\n",
    "The goals of this step are:\n",
    "1.  **Review the general structure and completeness of the dataset:**\n",
    "    * Are there missing values in key fields?\n",
    "    * Which fields are fully populated and which require cleaning?\n",
    "2.  **Examine the *Pattern Specifics* column in more detail:**\n",
    "    * How many unique pattern categories exist?\n",
    "    * Which patterns occur most frequently?\n",
    "    * *Note: Missing or \"Not documented\" values are excluded to focus on actionable data.*\n",
    "3.  **Evaluate the distribution of key variables related to medication errors:**\n",
    "    * Branch distribution (Air, Ground, etc.)\n",
    "    * Primary risk categories\n",
    "    * Types of medication involved (Medication 1, Medication 2)\n",
    "4.  **Generate early questions that will shape later analysis:**\n",
    "    * Which patterns appear most frequently?\n",
    "    * Do certain branches report more events for specific patterns?\n",
    "    * Are some risk types associated with particular medications?\n",
    "\n",
    "This mirrors the “initial EDA” step from the loan assignment, but here the emphasis is on **clinical pattern categories** rather than customer demographics. This step ensures we understand the landscape before aggregating or modeling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2uM8PmphV8_b",
    "outputId": "aabe0faa-caa6-4666-d1d0-462c461de745"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 4: Basic Exploratory Data Analysis (EDA)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Load Data\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    df_med = pd.read_excel('Krista 240726 Final.xlsx', sheet_name='Medication')\n",
    "except FileNotFoundError:\n",
    "    df_med = pd.read_csv('Krista 240726 Final.xlsx - Medication.csv')\n",
    "\n",
    "# 2. General Structure & Completeness\n",
    "# ---------------------------------------------------------\n",
    "print(\"--- Missing Values in Key Fields ---\")\n",
    "key_cols = ['Report ID', 'Source', 'Branch', 'Primary Risk', 'Risk Event', 'Medication 1', 'Medication 2', 'Pattern Specifics']\n",
    "# Filter for columns that actually exist in the dataframe\n",
    "existing_key_cols = [c for c in key_cols if c in df_med.columns]\n",
    "print(df_med[existing_key_cols].isna().sum())\n",
    "\n",
    "# 3. Pattern Specifics Analysis (Detailed & Filtered)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- Pattern Specifics Analysis (Valid Entries Only) ---\")\n",
    "\n",
    "# Filter out NaNs and generic \"missing\" placeholders\n",
    "valid_patterns = df_med['Pattern Specifics'].dropna()\n",
    "valid_patterns = valid_patterns[~valid_patterns.isin(['Not documented', 'Not Specified', 'Missing', 'NaN'])]\n",
    "\n",
    "print(f\"Unique Pattern Categories: {valid_patterns.nunique()}\")\n",
    "print(\"\\nTop 10 Most Frequent Patterns:\")\n",
    "print(valid_patterns.value_counts().head(10))\n",
    "\n",
    "# 4. Key Variable Distributions\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- Branch Distribution ---\")\n",
    "print(df_med['Branch'].value_counts(dropna=True))\n",
    "\n",
    "print(\"\\n--- Primary Risk Categories ---\")\n",
    "if 'Primary Risk' in df_med.columns:\n",
    "    print(df_med['Primary Risk'].value_counts(dropna=True))\n",
    "\n",
    "print(\"\\n--- Top 10 Medications Involved (Medication 1) ---\")\n",
    "print(df_med['Medication 1'].value_counts(dropna=True).head(10))\n",
    "\n",
    "# Optional: Visualizing the Top Patterns\n",
    "plt.figure(figsize=(10, 6))\n",
    "valid_patterns.value_counts().head(10).sort_values().plot(kind='barh', color='teal')\n",
    "plt.title('Top 10 Valid Medication Error Patterns')\n",
    "plt.xlabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSyRtTibV2La"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgbXl2yRWSxt"
   },
   "source": [
    "# Step 5 — Total Events by Pattern Specifics Category\n",
    "\n",
    "In this step, I analyze the **Pattern Specifics** categories to determine which clinical patterns account for the highest number of medication-related events across all certificates.\n",
    "\n",
    "The goals of this step are:\n",
    "1.  **Identify all certificate columns** containing event counts (e.g., AEL, GFL, AMR, MTC, REACH, and AMI).\n",
    "2.  **Compute the total number of events** for each *Pattern Specifics* category by summing across all certificates.\n",
    "3.  **Sort these totals** from highest to lowest to highlight the high-impact patterns.\n",
    "4.  **Visualize the top categories** in a bar chart for easier interpretation.\n",
    "\n",
    "This mirrors the \"risk category breakdown\" from the loan assignment, focusing on clinical event patterns to identify system-wide safety priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "id": "MD12gjTWWpaZ",
    "outputId": "126dad72-e165-4f04-bae8-97f665ae4696"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 5: Total Events by Pattern Specifics Category\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Load the Data\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    df_med = pd.read_excel('Krista 240726 Final.xlsx', sheet_name='Medication')\n",
    "except FileNotFoundError:\n",
    "    df_med = pd.read_csv('Krista 240726 Final.xlsx - Medication.csv')\n",
    "\n",
    "# 2. Prepare the Data (Filter valid patterns)\n",
    "# ---------------------------------------------------------\n",
    "# We drop rows where Pattern Specifics is missing to focus on known categories\n",
    "df_clean = df_med.dropna(subset=['Pattern Specifics']).copy()\n",
    "\n",
    "# 3. Compute Total Events by Category and Certificate\n",
    "# ---------------------------------------------------------\n",
    "# Create a cross-tabulation: Rows = Pattern Specifics, Cols = Source (Certificate)\n",
    "# This automatically identifies all certificates present in the dataset\n",
    "pattern_summary = pd.crosstab(df_clean['Pattern Specifics'], df_clean['Source'])\n",
    "\n",
    "# Add a 'Total Events' column by summing across all certificates\n",
    "pattern_summary['Total Events'] = pattern_summary.sum(axis=1)\n",
    "\n",
    "# 4. Sort and Select Top Categories\n",
    "# ---------------------------------------------------------\n",
    "# Sort by Total Events in descending order\n",
    "pattern_summary_sorted = pattern_summary.sort_values(by='Total Events', ascending=False)\n",
    "\n",
    "# Display the top 10 patterns\n",
    "print(\"--- Top 10 Pattern Specifics by Total Events ---\")\n",
    "print(pattern_summary_sorted[['Total Events']].head(10))\n",
    "\n",
    "# 5. Visualization: Bar Chart of Top Patterns\n",
    "# ---------------------------------------------------------\n",
    "# Select top 10 for plotting to keep the chart readable\n",
    "top_10_patterns = pattern_summary_sorted.head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Plot using the 'Total Events' column\n",
    "ax = sns.barplot(x=top_10_patterns['Total Events'], y=top_10_patterns.index, palette='viridis')\n",
    "\n",
    "plt.title('Top 10 Medication Error Patterns (All Certificates)', fontsize=16)\n",
    "plt.xlabel('Total Number of Events', fontsize=12)\n",
    "plt.ylabel('Pattern Specifics', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add count labels to the end of each bar\n",
    "for i, v in enumerate(top_10_patterns['Total Events']):\n",
    "    ax.text(v + 0.5, i, str(v), color='black', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpcbR3NDXGXB"
   },
   "source": [
    "# Step 6 — Create Structured “Pattern Specifics” Flags for Modeling\n",
    "\n",
    "In this step, I start transforming the free-text **Pattern Specifics** field into simple, structured variables that can be used in later analysis and modeling.\n",
    "\n",
    "The goals of this step are:\n",
    "* **Convert unstructured narrative text** into a few clinically meaningful indicator variables.\n",
    "* **Create yes/no flags** for major error themes that appear repeatedly in the data, such as:\n",
    "    * *Dosing* / maximum dose / volume problems\n",
    "    * *Wrong medication* / given instead of the intended drug\n",
    "    * *Protocol* / checklist or process-compliance issues\n",
    "* **Quantify how often** each type of pattern occurs across all medication events.\n",
    "* **Prepare the dataset** so that later steps (for example, decision trees or risk-scoring rules) can use these pattern flags as input features.\n",
    "\n",
    "This mirrors the “feature engineering” step from the loan assignment, but instead of creating financial variables, here I am engineering **clinical risk features** directly from the *Pattern Specifics* text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IDy1ClVXOA3",
    "outputId": "0860cbc6-1028-4bfe-aebd-cd95bf02432a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the Data (Ensuring a clean start for this step)\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    med = pd.read_excel('Krista 240726 Final.xlsx', sheet_name='Medication')\n",
    "except FileNotFoundError:\n",
    "    med = pd.read_csv('Krista 240726 Final.xlsx - Medication.csv')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 6: Feature Engineering - Creating Risk Flags\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Safety check: make sure the column exists\n",
    "if \"Pattern Specifics\" not in med.columns:\n",
    "    raise KeyError(\"The 'Pattern Specifics' column is not present in the Medication sheet.\")\n",
    "\n",
    "# 1. Create simple pattern flags (1 = pattern present, 0 = not present)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Dosing Errors\n",
    "# Catches \"dosing\", \"max dose\", \"overdose\", \"underdose\", etc.\n",
    "med[\"Flag_Dosing_Error\"] = med[\"Pattern Specifics\"].str.contains(\n",
    "    r\"dosing|max dose|max\\.? dose|volume|overdose|underdose\",\n",
    "    case=False, na=False\n",
    ").astype(int)\n",
    "\n",
    "# Wrong Medication\n",
    "# Catches \"wrong med\", \"incorrect medication\", \"instead of\"\n",
    "med[\"Flag_Wrong_Med\"] = med[\"Pattern Specifics\"].str.contains(\n",
    "    r\"wrong med|wrong medication|instead of|incorrect medication\",\n",
    "    case=False, na=False\n",
    ").astype(int)\n",
    "\n",
    "# Protocol/Compliance Errors\n",
    "# Catches \"protocol\", \"checklist\", \"policy\", \"procedure\"\n",
    "med[\"Flag_Protocol_Error\"] = med[\"Pattern Specifics\"].str.contains(\n",
    "    r\"protocol|checklist|policy|procedure\",\n",
    "    case=False, na=False\n",
    ").astype(int)\n",
    "\n",
    "# 2. Quick frequency check for each flag\n",
    "# ---------------------------------------------------------\n",
    "flag_cols = [\"Flag_Dosing_Error\", \"Flag_Wrong_Med\", \"Flag_Protocol_Error\"]\n",
    "\n",
    "print(\"New pattern-flag columns added:\")\n",
    "print(flag_cols)\n",
    "\n",
    "print(\"\\nEvent counts by flag (1 = pattern present):\")\n",
    "for col in flag_cols:\n",
    "    count = med[col].sum()\n",
    "    print(f\"{col}: {count} events\")\n",
    "\n",
    "# 3. Optional: show first few rows with the new flags\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nFirst 10 rows with new Pattern Specifics flags:\")\n",
    "cols_to_display = [\"Report ID\", \"Branch\", \"Primary Risk\", \"Risk Event\",\n",
    "                   \"Medication 1\", \"Pattern Specifics\"] + flag_cols\n",
    "print(med[cols_to_display].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJ92mVyKPLeN"
   },
   "source": [
    "#Step 7 — Pattern Specifics by Branch (Air vs Ground)\n",
    "\n",
    "In this step, I move from simple counts of Pattern Specifics to a bivariate view that relates error patterns to transport modality (Air vs Ground).\n",
    "\n",
    "Goals of this step:\n",
    "\n",
    "• Identify which detailed **medication** error patterns (Pattern Specifics) occur most frequently in the Medication sheet.\n",
    "• Compare how often these high-volume patterns appear in Air vs Ground branches.\n",
    "• Highlight any patterns that are disproportionately associated with a single branch, which may suggest targeted safety or education needs.\n",
    "\n",
    "Approach:\n",
    "\n",
    "1. Use the Medication dataset (med) to find the most common values in the Pattern Specifics column.\n",
    "2. Select the top N patterns (for example, the 10 most frequent) to keep the visualization interpretable.\n",
    "3. Build a cross-tabulation (crosstab) of Pattern Specifics (rows) by Branch (columns: Air, Ground).\n",
    "4. Visualize this crosstab as a heatmap so that branch–pattern combinations with higher counts are immediately visible.\n",
    "\n",
    "This step mirrors the “bivariate EDA” section from the loan assignment, but instead of comparing financial variables to Personal_Loan, I am comparing clinical pattern types to the operational branch (Air vs Ground). The result is an early branch-specific risk profile for the most common medication error patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VT1zz5jfPNQS",
    "outputId": "214ac724-48df-491a-deb6-03b1e8ac0a5d"
   },
   "outputs": [],
   "source": [
    "# Step 7 — Pattern Specifics by Branch (Air vs Ground)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# 1. Drop missing Pattern Specifics and find the most frequent patterns\n",
    "pattern_counts = (\n",
    "    med[\"Pattern Specifics\"]\n",
    "    .dropna()\n",
    "    .value_counts()\n",
    ")\n",
    "\n",
    "# Choose how many top patterns to display\n",
    "n_top = 10\n",
    "top_patterns = pattern_counts.head(n_top).index.tolist()\n",
    "\n",
    "print(f\"Top {n_top} Pattern Specifics in the Medication dataset:\")\n",
    "print(pattern_counts.head(n_top))\n",
    "\n",
    "# 2. Filter Medication data to only those top patterns\n",
    "med_top = med[med[\"Pattern Specifics\"].isin(top_patterns)].copy()\n",
    "\n",
    "# 3. Create a crosstab: Pattern Specifics (rows) x Branch (columns)\n",
    "ct_branch_pattern = pd.crosstab(\n",
    "    med_top[\"Pattern Specifics\"],\n",
    "    med_top[\"Branch\"]\n",
    ")\n",
    "\n",
    "print(\"\\nCrosstab of top Pattern Specifics by Branch (Air vs Ground):\")\n",
    "display(ct_branch_pattern)\n",
    "\n",
    "# 4. Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    ct_branch_pattern,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"YlOrRd\"\n",
    ")\n",
    "plt.title(f\"Top {n_top} Medication Error Patterns by Branch (Air vs Ground)\")\n",
    "plt.xlabel(\"Branch\")\n",
    "plt.ylabel(\"Pattern Specifics\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArJ8k6llRY0U"
   },
   "source": [
    "# Step 7A — Certificate Analysis and Visualization\n",
    "\n",
    "In this step, we analyze the medication errors by **Certificate** (Source). We will generate:\n",
    "* A **Bar Chart** to compare the total volume of reported errors for each certificate.\n",
    "* A **Heat Map** to identify the **Top 10 most frequent error patterns** for each certificate.\n",
    "\n",
    "The specific certificates being compared are: **AEL, GFL, MTC, REACH, and AMR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vU3vBGoJRak4",
    "outputId": "11a6c65c-b916-4891-a09e-d6504379a7fb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Load the Data\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    df_med = pd.read_excel('Krista 240726 Final.xlsx', sheet_name='Medication')\n",
    "    print(\"Success: Loaded data from Excel file.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df_med = pd.read_csv('Krista 240726 Final.xlsx - Medication.csv')\n",
    "        print(\"Success: Loaded data from CSV file.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"ERROR: File not found. Please check your file name.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Data Filtering & Preparation\n",
    "# ---------------------------------------------------------\n",
    "# REMOVED 'AMI' from this list\n",
    "target_certificates = ['AEL', 'GFL', 'MTC', 'REACH', 'AMR']\n",
    "\n",
    "# Filter the dataframe to include only these sources\n",
    "df_filtered = df_med[df_med['Source'].isin(target_certificates)].copy()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Chart: Total Medication Errors by Certificate\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Create Bar Chart\n",
    "ax = sns.countplot(data=df_filtered, x='Source', order=target_certificates, palette='viridis')\n",
    "\n",
    "# Formatting\n",
    "plt.title('Total Medication Errors by Certificate', fontsize=16)\n",
    "plt.ylabel('Count of Errors')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add numeric labels on top of bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}',\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 9), textcoords='offset points')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. FOCUSED Heat Map: Top 10 Patterns Only\n",
    "# ---------------------------------------------------------\n",
    "# 1. Count all patterns to find the Top 10\n",
    "pattern_counts = df_filtered['Pattern Specifics'].value_counts()\n",
    "top_10_patterns = pattern_counts.head(10).index\n",
    "\n",
    "# 2. Filter data to only those top 10 patterns\n",
    "df_heatmap_focused = df_filtered[df_filtered['Pattern Specifics'].isin(top_10_patterns)]\n",
    "\n",
    "# 3. Create the cross-tabulation table\n",
    "heatmap_data = pd.crosstab(df_heatmap_focused['Pattern Specifics'], df_heatmap_focused['Source'])\n",
    "\n",
    "# 4. Reorder columns to match our specific list (AEL, GFL, etc.)\n",
    "heatmap_data = heatmap_data.reindex(columns=target_certificates)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 5. Generate Heat Map\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='g', cmap='Reds', linewidths=.5)\n",
    "\n",
    "plt.title('Top 10 Medication Error Patterns by Certificate', fontsize=16)\n",
    "plt.xlabel('Certificate', fontsize=12)\n",
    "plt.ylabel('Error Pattern', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKeulCjJR-iK"
   },
   "source": [
    "# Step 8 — Analysis of Medications Involved\n",
    "\n",
    "Now that we have analyzed errors by Certificate, we proceed to analyze the **Medications** involved in these events.\n",
    "\n",
    "In this step, we will:\n",
    "* Identify the **Top 10 most frequently involved medications** in reported errors.\n",
    "* Visualize the frequency of these medications using a **Bar Chart**.\n",
    "* Use a **Heat Map** to see if specific medications are prone to specific types of error patterns (e.g., Are *Fentanyl* errors mostly dosing errors?).\n",
    "\n",
    "*Note: We continue to focus on the key certificates (AEL, GFL, MTC, REACH, AMR) defined in the previous step.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IIWEdLJKSA51",
    "outputId": "f3396f13-fc1c-49f3-9be1-b7c8f5548755"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 8: Medication Involved Analysis\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Prepare the Data\n",
    "# ---------------------------------------------------------\n",
    "# Ensure we are using the filtered data from Step 2\n",
    "# (If you are running this in a new session, make sure 'df_filtered' is defined as in Step 2)\n",
    "# We filter for 'Medication 1' to ignore empty entries if any\n",
    "df_meds_analysis = df_filtered[df_filtered['Medication 1'].notna()].copy()\n",
    "\n",
    "# 2. Identify Top 10 Medications\n",
    "# ---------------------------------------------------------\n",
    "med_counts = df_meds_analysis['Medication 1'].value_counts()\n",
    "top_10_meds = med_counts.head(10).index\n",
    "print(f\"Top 10 Medications Identified: {list(top_10_meds)}\")\n",
    "\n",
    "# Filter data to only include these top 10 medications\n",
    "df_top_meds = df_meds_analysis[df_meds_analysis['Medication 1'].isin(top_10_meds)]\n",
    "\n",
    "# 3. Chart: Frequency of Errors by Medication\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.countplot(data=df_top_meds, x='Medication 1', order=top_10_meds, palette='mako')\n",
    "\n",
    "plt.title('Top 10 Medications Involved in Errors', fontsize=16)\n",
    "plt.xlabel('Medication', fontsize=12)\n",
    "plt.ylabel('Count of Errors', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add count labels\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}',\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 9), textcoords='offset points')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Heat Map: Top 10 Medications vs. Top 10 Patterns\n",
    "# ---------------------------------------------------------\n",
    "# We already have 'top_10_meds'. Let's ensure we also focus on 'top_10_patterns' from Step 2.\n",
    "# (Recalculating top 10 patterns here to ensure code is self-contained)\n",
    "pattern_counts_step3 = df_top_meds['Pattern Specifics'].value_counts()\n",
    "top_10_patterns_step3 = pattern_counts_step3.head(10).index\n",
    "\n",
    "# Filter for both Top Meds AND Top Patterns\n",
    "df_heatmap_meds = df_top_meds[df_top_meds['Pattern Specifics'].isin(top_10_patterns_step3)]\n",
    "\n",
    "# Create Cross-tab\n",
    "heatmap_med_data = pd.crosstab(df_heatmap_meds['Pattern Specifics'], df_heatmap_meds['Medication 1'])\n",
    "\n",
    "# Reorder columns to match the sorted Top 10 Meds order\n",
    "heatmap_med_data = heatmap_med_data.reindex(columns=top_10_meds)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_med_data, annot=True, fmt='g', cmap='Blues', linewidths=.5)\n",
    "\n",
    "plt.title('Heat Map: Top 10 Medications vs. Top Error Patterns', fontsize=16)\n",
    "plt.xlabel('Medication', fontsize=12)\n",
    "plt.ylabel('Error Pattern', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhBYaU0xX8qj"
   },
   "source": [
    "# Step 9 — Exploratory Data Analysis (EDA): Bivariate Analysis\n",
    "\n",
    "In this step, we investigate relationships between key variables and the **Target Variable (Outcome)**. This corresponds to the \"Bivariate Analysis\" phase of the loan project, where predictors were compared against the `Personal_Loan` status.\n",
    "\n",
    "**Goals:**\n",
    "1.  **Define the Target:** Convert the free-text `Outcome` field into a structured category (**Severity**: 'Critical/Severe', 'No Harm/Stable', etc.).\n",
    "2.  **Analyze Certificate vs. Outcome:** Determine if specific certificates (Sources) have a higher rate of severe errors.\n",
    "3.  **Analyze Medication vs. Outcome:** Identify which medications are most frequently associated with adverse outcomes.\n",
    "4.  **Analyze Error Patterns vs. Outcome:** See if certain error types (e.g., Dosing Errors) correlate with higher severity.\n",
    "\n",
    "This step identifies the \"drivers\" of severe errors, just as bivariate analysis identifies the drivers of loan acceptance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G_vN2J0QYBAa",
    "outputId": "9b9f55a4-229e-46a0-f6e0-9610a4c62708"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 9: Bivariate Analysis\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Prepare Data & Define Target Variable\n",
    "# ---------------------------------------------------------\n",
    "# Ensure we are working with the latest data\n",
    "if 'med' in locals():\n",
    "    df_bi = med.copy()\n",
    "else:\n",
    "    # Fallback load if 'med' isn't defined from previous steps\n",
    "    try:\n",
    "        df_bi = pd.read_excel('Krista 240726 Final.xlsx', sheet_name='Medication')\n",
    "    except:\n",
    "        df_bi = pd.read_csv('Krista 240726 Final.xlsx - Medication.csv')\n",
    "\n",
    "# Function to categorize Outcome Severity (The \"Target\")\n",
    "def categorize_severity(text):\n",
    "    text = str(text).lower()\n",
    "    # Define severe keywords\n",
    "    severe_terms = ['died', 'death', 'expired', 'cpr', 'arrest', 'hypoxia', 'intubated', 'seizure']\n",
    "    # Define stable/no-harm keywords\n",
    "    stable_terms = ['stable', 'no adverse', 'resolved', 'prevented', 'normal', 'unchanged']\n",
    "\n",
    "    if any(x in text for x in severe_terms):\n",
    "        return 'Critical/Severe'\n",
    "    elif any(x in text for x in stable_terms):\n",
    "        return 'No Harm/Stable'\n",
    "    elif 'not documented' in text or text == 'nan':\n",
    "        return 'Unknown'\n",
    "    else:\n",
    "        return 'Monitor/Intervention' # Intermediate category for things like \"given Narcan\", \"hypotension\"\n",
    "\n",
    "df_bi['Severity_Category'] = df_bi['Outcome'].apply(categorize_severity)\n",
    "\n",
    "print(\"--- Target Variable Distribution (Severity) ---\")\n",
    "print(df_bi['Severity_Category'].value_counts())\n",
    "\n",
    "# 2. Analysis 1: Certificate (Source) vs. Severity\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Create a crosstab of counts\n",
    "source_outcome = pd.crosstab(df_bi['Source'], df_bi['Severity_Category'])\n",
    "# Normalize to show percentages (Heatmap of Risk)\n",
    "sns.heatmap(source_outcome, annot=True, fmt='d', cmap='YlOrRd', linewidths=.5)\n",
    "plt.title('Bivariate: Certificate (Source) vs. Outcome Severity')\n",
    "plt.ylabel('Certificate')\n",
    "plt.xlabel('Outcome Severity')\n",
    "plt.show()\n",
    "\n",
    "# 3. Analysis 2: Top 10 Medications vs. Severity\n",
    "# ---------------------------------------------------------\n",
    "# Filter for top 10 meds\n",
    "top_meds_list = df_bi['Medication 1'].value_counts().head(10).index\n",
    "df_top_meds = df_bi[df_bi['Medication 1'].isin(top_meds_list)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df_top_meds, y='Medication 1', hue='Severity_Category',\n",
    "              order=top_meds_list, palette='magma')\n",
    "plt.title('Bivariate: Top 10 Medications by Outcome Severity')\n",
    "plt.xlabel('Count of Events')\n",
    "plt.ylabel('Medication')\n",
    "plt.legend(title='Severity', loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# 4. Analysis 3: Pattern Flags vs. Severity (Correlation)\n",
    "# ---------------------------------------------------------\n",
    "# We check if our created flags (Dosing, Wrong Med, Protocol) correlate with Critical outcomes\n",
    "# Create a binary target for correlation: 1 = Critical/Severe, 0 = Other\n",
    "df_bi['Is_Critical'] = (df_bi['Severity_Category'] == 'Critical/Severe').astype(int)\n",
    "\n",
    "# Ensure flags exist (from Step 6)\n",
    "flags = ['Flag_Dosing_Error', 'Flag_Wrong_Med', 'Flag_Protocol_Error']\n",
    "if all(col in df_bi.columns for col in flags):\n",
    "    print(\"\\n--- Severe Outcome Rate by Error Type ---\")\n",
    "    for flag in flags:\n",
    "        # Calculate percentage of severe cases for each flag\n",
    "        rate = df_bi[df_bi[flag] == 1]['Is_Critical'].mean() * 100\n",
    "        print(f\"Error Type: {flag:<20} | Severe Rate: {rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxROyM5abQKQ"
   },
   "source": [
    "# Step 10 — Clinical Data Preprocessing\n",
    "\n",
    "In this step, we transform our raw medication error data into a structured format suitable for risk modeling. Unlike standard business datasets, clinical data often requires specific handling for free-text outcomes and rare events.\n",
    "\n",
    "**Key Actions:**\n",
    "1.  **Define the Target (`Is_Critical`):** We convert the free-text `Outcome` field into a binary target.\n",
    "    * **1 (Critical):** Outcomes involving harm, intervention, or distress (e.g., \"hypoxia\", \"monitoring\", \"intervention\").\n",
    "    * **0 (Stable/No Harm):** Outcomes where the patient remained stable or no effect was observed.\n",
    "2.  **Feature Engineering:**\n",
    "    * **Medication Grouping:** There are hundreds of unique medications. To prevent the model from getting \"confused\" by rare drugs, we keep the **Top 10 most frequent medications** and group the rest as \"Other\".\n",
    "    * **Risk Flags:** We incorporate the boolean flags created in Step 6 (Dosing Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1qU93zFbbFX",
    "outputId": "086296f4-e0eb-4c97-b42d-e652722f840a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 10: Clinical Data Preprocessing\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Define the Target: Critical vs. Non-Critical\n",
    "# ---------------------------------------------------------\n",
    "# In clinical risk, we want to predict 'Severe' outcomes (1) vs 'Stable/Minor' (0)\n",
    "# (This logic pulls from your 'Outcome' text analysis in Step 6)\n",
    "def categorize_severity(text):\n",
    "    text = str(text).lower()\n",
    "    severe_terms = ['died', 'death', 'expired', 'cpr', 'arrest', 'hypoxia', 'intubated', 'seizure']\n",
    "    if any(x in text for x in severe_terms): return 1  # Critical Outcome\n",
    "    return 0  # No Harm/Stable/Monitoring\n",
    "\n",
    "# Apply to fresh data if needed\n",
    "if 'med' in locals():\n",
    "    df_model = med.copy()\n",
    "else:\n",
    "    # Reload if variables are lost\n",
    "    df_model = pd.read_csv('Krista 240726 Final.xlsx - Medication.csv') # Or use read_excel if preferred\n",
    "    # Re-create flags from Step 6 if they aren't there\n",
    "    df_model[\"Flag_Dosing_Error\"] = df_model[\"Pattern Specifics\"].str.contains(r\"dosing|max dose|volume\", case=False, na=False).astype(int)\n",
    "    df_model[\"Flag_Wrong_Med\"] = df_model[\"Pattern Specifics\"].str.contains(r\"wrong med|instead of\", case=False, na=False).astype(int)\n",
    "    df_model[\"Flag_Protocol_Error\"] = df_model[\"Pattern Specifics\"].str.contains(r\"protocol|checklist\", case=False, na=False).astype(int)\n",
    "\n",
    "df_model['Is_Critical'] = df_model['Outcome'].apply(categorize_severity)\n",
    "\n",
    "# 2. Feature Selection: Prioritize Clinical Context\n",
    "# ---------------------------------------------------------\n",
    "# Group rare medications into 'Other' to focus on systemic drug risks\n",
    "top_10_meds = df_model['Medication 1'].value_counts().nlargest(10).index\n",
    "df_model['Med_Grouped'] = df_model['Medication 1'].apply(lambda x: x if x in top_10_meds else 'Other')\n",
    "\n",
    "# Select features that drive process improvement\n",
    "feature_cols = [\n",
    "    'Source',               # Who made the error? (Certificate)\n",
    "    'Branch',               # Where did it happen? (Air vs. Ground)\n",
    "    'Med_Grouped',          # Which drug? (High risk meds)\n",
    "    'Flag_Dosing_Error',    # Is it a dosing issue?\n",
    "    'Flag_Wrong_Med',       # Is it a wrong drug issue?\n",
    "    'Flag_Protocol_Error'   # Is it a protocol violation?\n",
    "]\n",
    "\n",
    "X = df_model[feature_cols]\n",
    "y = df_model['Is_Critical']\n",
    "\n",
    "# 3. Encoding & Splitting\n",
    "# ---------------------------------------------------------\n",
    "# Convert categories (Source, Branch) into binary flags for the model\n",
    "X = pd.get_dummies(X, columns=['Source', 'Branch', 'Med_Grouped'], drop_first=True)\n",
    "\n",
    "# Split Data: Stratify is CRITICAL here to maintain the ratio of severe events in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training Data: {X_train.shape[0]} events\")\n",
    "print(f\"Testing Data:  {X_test.shape[0]} events\")\n",
    "print(f\"Critical Events in Train Set: {y_train.sum()} ({y_train.mean():.1%} of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGm_hUqjbgI9"
   },
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 11: Clinical Risk Model (Decision Tree)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Train the Model with Class Balancing\n",
    "# 'class_weight=\"balanced\"' forces the model to pay attention to the rare 'Critical' events\n",
    "risk_model = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "risk_model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Predictions\n",
    "y_pred = risk_model.predict(X_test)\n",
    "\n",
    "# 3. Clinical Evaluation\n",
    "print(\"--- Model Performance ---\")\n",
    "# Accuracy is less important here than Recall (catching the bad events)\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred):.2%}\")\n",
    "print(f\"Critical Event Capture Rate (Recall): {recall_score(y_test, y_pred):.2%}\")\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "# Format: [ True Negatives   False Positives (False Alarms) ]\n",
    "#         [ False Negatives (Missed!)   True Positives (Caught) ]\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n--- Feature Importance (What drives risk?) ---\")\n",
    "# Identify which clinical factors (Flags, Certs, Meds) drive severe outcomes\n",
    "feat_importances = pd.Series(risk_model.feature_importances_, index=X_train.columns)\n",
    "print(feat_importances.nlargest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d64SzIOUbq92",
    "outputId": "1e6bc0b8-10ed-411d-ca74-4f65123136f2"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 11: Clinical Risk Model (Decision Tree)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Train the Model with Class Balancing\n",
    "# 'class_weight=\"balanced\"' tells the model: \"Critical errors are rare but expensive. Prioritize finding them.\"\n",
    "risk_model = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "risk_model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Predictions\n",
    "y_pred = risk_model.predict(X_test)\n",
    "\n",
    "# 3. Clinical Evaluation\n",
    "print(\"--- Model Performance ---\")\n",
    "# Accuracy is less important here than Recall (catching the bad events)\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred):.2%}\")\n",
    "print(f\"Critical Event Capture Rate (Recall): {recall_score(y_test, y_pred):.2%}\")\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "# Format: [ True Negatives   False Positives (False Alarms) ]\n",
    "#         [ False Negatives (Missed!)   True Positives (Caught) ]\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n--- Feature Importance (What drives risk?) ---\")\n",
    "# Identify which clinical factors (Flags, Certs, Meds) drive severe outcomes\n",
    "feat_importances = pd.Series(risk_model.feature_importances_, index=X_train.columns)\n",
    "print(feat_importances.nlargest(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gR6lER7bzy5"
   },
   "source": [
    "# Step 12 — Model Improvement (Tuning for Safety)\n",
    "\n",
    "The initial model might be too complex or \"noisy.\" In this step, we use **Hyperparameter Tuning** to find the optimal tree structure that balances accuracy with simplicity.\n",
    "\n",
    "**Optimization Goals:**\n",
    "* **Metric:** We optimize for **ROC-AUC** (Area Under the Curve). This is a better metric for imbalanced data than simple accuracy because it measures how well the model distinguishes between \"Critical\" and \"Stable\" events across all probability thresholds.\n",
    "* **Tree Depth:** We test different tree depths to find simple, robust rules that define high-risk scenarios.\n",
    "* **Rule Extraction:** Finally, we extract the logic from the best tree to provide actionable \"Safety Rules\" for the clinical team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5shTtHP1b0zw",
    "outputId": "522f0ddc-4ec9-4d96-ce1c-672c2e72b21e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 12: Improving Risk Detection (Pruning)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Setup Grid Search\n",
    "# We test different tree depths to find the \"sweet spot\" for safety rules\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 7, 10],\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'class_weight': ['balanced'] # Always keep balanced for medical risk\n",
    "}\n",
    "\n",
    "# Optimize for ROC_AUC (ability to distinguish severe from non-severe)\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 2. Best Model Results\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Ruleset Depth: {grid_search.best_params_['max_depth']}\")\n",
    "print(f\"Best ROC-AUC Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 3. Visualize the \"Safety Rules\"\n",
    "# This text printout shows exactly which conditions lead to High Risk\n",
    "rules = export_text(best_model, feature_names=list(X_train.columns))\n",
    "print(\"\\n--- Clinical Risk Rules Generated by Model ---\")\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF-n_n0WcQcA"
   },
   "source": [
    "# Step 13 — Actionable Insights & Clinical Recommendations\n",
    "\n",
    "Based on the Decision Tree analysis and risk modeling, we have identified the primary drivers of critical medication errors. Below are the key findings and specific recommendations to improve patient safety.\n",
    "\n",
    "### **1. Key Drivers of Critical Outcomes**\n",
    "Our model identified the following factors as the strongest predictors of a Severe/Critical outcome:\n",
    "* **Dosing Errors (Highest Impact):** Errors involving \"max dose,\" \"overdose,\" or \"volume\" calculations are the single biggest predictor of a severe patient outcome.\n",
    "* **High-Risk Medications:** Specific drugs, particularly **Fentanyl** and **Ketamine**, appear frequently in the high-risk branches of the decision tree.\n",
    "* **Protocol Deviations:** Events flagged as \"protocol\" or \"checklist\" violations have a high correlation with adverse events, suggesting process failures rather than just individual mistakes.\n",
    "\n",
    "### **2. Recommendations for Intervention**\n",
    "\n",
    "**A. Target Dosing Calculations for High-Risk Meds**\n",
    "* **Finding:** Dosing errors involving sedatives/analgesics (Fentanyl, Ketamine) frequently lead to hypoxia or arrest.\n",
    "* **Recommendation:** Implement a **mandatory \"High-Risk Double Check\"** in the ePCR system. When a crew selects Fentanyl or Ketamine, the system should require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwdK5RgqcU-R",
    "outputId": "faae2fea-affd-49fd-d0bc-fd4b6601ee09"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 13: Generating Clinical Insights\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Visualize the Safety Rules (Decision Logic)\n",
    "# We extract the top rules from the tree to see exactly where the risk \"splits\" occur.\n",
    "# (e.g., \"If Flag_Dosing_Error > 0.5, then Class: 1 (Critical)\")\n",
    "tree_rules = export_text(best_model, feature_names=list(X_train.columns), max_depth=3)\n",
    "print(\"--- Key Clinical Risk Rules (Decision Tree) ---\")\n",
    "print(tree_rules)\n",
    "\n",
    "# 2. Risk Profiling: Compare Critical vs. Non-Critical Events\n",
    "# We calculate the mean values for our flags to see how much more frequent they are in critical cases.\n",
    "# (Re-combining X and y for analysis)\n",
    "analysis_df = X_train.copy()\n",
    "analysis_df['Is_Critical'] = y_train\n",
    "\n",
    "print(\"\\n--- Risk Profile: Frequency of Errors in Critical vs. Stable Cases ---\")\n",
    "# Group by Outcome (0=Stable, 1=Critical) and get the average of the error flags (which acts as a percentage)\n",
    "risk_profile = analysis_df.groupby('Is_Critical')[['Flag_Dosing_Error', 'Flag_Wrong_Med', 'Flag_Protocol_Error']].mean() * 100\n",
    "risk_profile.index = ['Stable/Minor (0)', 'Critical/Severe (1)']\n",
    "print(risk_profile.round(1).astype(str) + '%')\n",
    "\n",
    "# 3. High-Risk Medication Analysis\n",
    "# Which specific drugs have the highest % rate of critical outcomes?\n",
    "print(\"\\n--- Critical Outcome Rate by Medication (Top 5 High Risk) ---\")\n",
    "# We use the original dataframe to get the raw medication names back\n",
    "med_risk = df_model.groupby('Medication 1')['Is_Critical'].mean() * 100\n",
    "# Filter for meds with at least 10 events to ensure statistical relevance\n",
    "med_counts = df_model['Medication 1'].value_counts()\n",
    "common_meds = med_counts[med_counts >= 10].index\n",
    "print(med_risk[common_meds].sort_values(ascending=False).head(5).round(1).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "x-8O0uBwqjdS",
    "outputId": "880b48d4-8285-4337-a4ac-b3d3c6965129"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Visualizing the Decision Tree\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# We export the 'best_model' we trained in Step 9\n",
    "dot_data = export_graphviz(\n",
    "    best_model,\n",
    "    out_file=None,\n",
    "    feature_names=list(X_train.columns),\n",
    "    class_names=['Stable', 'Critical'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True,\n",
    "    max_depth=3 # Limit depth for readability\n",
    ")\n",
    "\n",
    "# Render the graph\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "rMpaIpfhpVde",
    "outputId": "060c89c5-7f5a-4433-985d-41d3b900e355"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# (Assuming 'df_target' and 'target_certs' are defined)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "colors = {'AEL': '#1f77b4', 'GFL': '#ff7f0e', 'MTC': '#2ca02c', 'REACH': '#d62728', 'AMR': '#9467bd'}\n",
    "\n",
    "for cert in target_certs:\n",
    "    subset = df_target[df_target['Source'] == cert]\n",
    "    if len(subset) < 5: continue\n",
    "\n",
    "    # Resample\n",
    "    ts = subset.set_index('Date').resample('MS').size()\n",
    "\n",
    "    try:\n",
    "        # Forecast\n",
    "        model = ExponentialSmoothing(ts, trend='add', seasonal=None, initialization_method=\"estimated\").fit()\n",
    "        forecast = model.forecast(6)\n",
    "        total_err = forecast.sum()\n",
    "\n",
    "        label_str = f\"{cert} (Proj: {total_err:.1f})\"\n",
    "\n",
    "        # Plot smoothed history\n",
    "        plt.plot(ts.index, ts.rolling(3).mean(), color=colors[cert], linewidth=2, label=label_str)\n",
    "        # Plot forecast\n",
    "        plt.plot(forecast.index, forecast, color=colors[cert], linestyle='--', linewidth=2, alpha=0.8)\n",
    "        # End dot\n",
    "        plt.plot(forecast.index[-1], forecast.iloc[-1], marker='o', color=colors[cert], markersize=5)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Formatting\n",
    "plt.title('Projected Fentanyl/Ketamine Errors (Next 6 Months)', fontsize=18, weight='bold')\n",
    "plt.ylabel('Monthly Error Count', fontsize=12)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# CUSTOM Y-AXIS (Max Height 3.0 with 0.5 intervals)\n",
    "# We set the ticks exactly from 0 to 3.0\n",
    "plt.yticks(np.arange(0, 3.1, 0.5))\n",
    "plt.ylim(bottom=0, top=3.0)\n",
    "\n",
    "# Legend Logic\n",
    "plt.plot([], [], color='black', linewidth=1, label='─ History (Trend)')\n",
    "plt.plot([], [], color='black', linestyle='--', linewidth=1, label='--- Forecast')\n",
    "plt.legend(loc='upper left', fontsize=11, frameon=True, shadow=True, title=\"Certificate (6-Mo Projection)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySO7fZ5oqL4s"
   },
   "source": [
    "# --- FINAL EXECUTIVE SUMMARY: Clinical Risk & Operational Forecast ---\n",
    "\n",
    "### **1. Project Scope & Objectives**\n",
    "This analysis aimed to transform **558 medication error reports** (across AEL, AMI, GFL, MTC, REACH, AMR) into actionable safety intelligence. We utilized a dual-modeling approach:\n",
    "* **Risk Modeling (Decision Tree):** To identify *which* errors cause severe patient harm (Critical/Severe Outcome).\n",
    "* **Volume Forecasting (Time Series):** To predict *how many* high-risk errors will occur in the next 6 months.\n",
    "\n",
    "### **2. Clinical Risk Findings (The \"What\")**\n",
    "Our predictive model (98.4% Accuracy, **94.4% Recall**) successfully identified the primary drivers of critical patient outcomes:\n",
    "* **Primary Driver:** **Dosing Errors** are the single leading cause of harm (33.3% impact on severity). Specifically, errors involving *\"Limit per kg exceeded\"* and *\"Volume Calculation\"* outweigh equipment failures in terms of risk.\n",
    "* **High-Risk Agents:** **Ketamine (90 events)** and **Fentanyl (66 events)** combined account for **28%** of all reported errors. These drugs appear most frequently in the \"high-risk\" branches of the decision tree.\n",
    "* **Protocol Drift:** The **Air Branch** showed a higher correlation with protocol deviations (15.5% impact) compared to Ground, suggesting environmental complexity is a factor in safety checklist adherence.\n",
    "\n",
    "### **3. Operational Forecast (The \"Where\" & \"When\")**\n",
    "*Based on Yoplait-style Time Series Forecasting (Holt’s Linear Model)*\n",
    "\n",
    "We projected the volume of high-risk medication errors (Fentanyl/Ketamine) for the upcoming 6-month period to prioritize resource allocation:\n",
    "\n",
    "* **High Priority Targets (Immediate Intervention):**\n",
    "    * **AMR:** Projected **7.9** errors. (Note: Data is volatile, RMSE 1.40, indicating sporadic spikes).\n",
    "    * **AEL:** Projected **7.1** errors. (Note: High confidence forecast, RMSE 0.59).\n",
    "* **Lower Priority / Monitoring:**\n",
    "    * **GFL:** Projected **6.4** errors.\n",
    "    * **MTC:** Projected **4.0** errors.\n",
    "    * **REACH:** Projected **0.9** errors (Trending positively downwards).\n",
    "\n",
    "### **4. Strategic Recommendations**\n",
    "Based on the convergence of clinical risk data and operational forecasts, we recommend:\n",
    "\n",
    "1.  **Implement \"Hard Stop\" Logic (Priority: High)**\n",
    "    * **Data Rationale:** Fentanyl and Ketamine are the strongest predictors of severe outcomes.\n",
    "    * **Action:** Configure the ePCR to require a mandatory peer-verify checkbox for these two agents. **Roll this out to AMR and AEL first**, as they account for the majority of the projected error volume.\n",
    "\n",
    "2.  **Target \"Weight-Based\" Training (Priority: Medium)**\n",
    "    * **Data Rationale:** \"Limit per kg exceeded\" was identified as a critical failure point in the decision tree.\n",
    "    * **Action:** Launch a focused training module on pediatric and weight-based calculations to address this specific skill gap.\n",
    "\n",
    "3.  **Air Medical Protocol Review (Priority: Medium)**\n",
    "    * **Data Rationale:** Protocol deviations are the second highest driver of risk (15.5%) and are more prevalent in the Air branch (67% of total reports).\n",
    "    * **Action:** Simplify in-flight checklists for Air Medical crews to reduce cognitive load and improve adherence during transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52bL4CWBqNHV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
